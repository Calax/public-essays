# Как мы строили систему отчетности в одной компании _P_.

Здесь я попытался собрать воедино информацию о том, как мы строили отчетность в некоторой компании _P_. Что вышло удачно, что не очень. И мое видение подобных систем в целом.

## Три уровня логической архитектуры данных

Логически мы выстраивали трехуровневую логическую архитектуру.

![three-layer-diagram](https://drive.google.com/uc?export=view&id=1n5E-5kfZIkgvz7GAGUyUAVM_PeCyCLMF)

Первым принимал данные слой долгострочного хранения первички, он же _Исторический_. Здесь скапливалась исходная информация. Основное назначение - источник правды. Мы должны были обеспечить возможность расследования любой ситуации спустя любое количество времени. Так же по данным из этого источника рассчитывались исторические показатели для следующих слоев. 

Следом располагался _аналитический_ слой. Данные на этом слое формировали реляционную модель предметной области. С этим слоем работают аналитики (запускают ad-hoc запросы по текущим задачам). Для этого слоя важны  логическая структура (интуитивная) и документация, т.е. человеку, пишущему запрос, должно быть понятно, где взять нужную ему информация, как соединить данные в разных таблицах. В этот слой образается много разноплановых запросов, поэтому стараемся организовать данные (подбираем архитектуру хранилища и инструменты) таким образом, чтобы запросы выполнялись за предсказуемое время.

Финальным слоем был слой витрин, _презентационный_. Тут располагались предрасчитанные данные под конкретные задачи, например, графики в админке или контент для опреденных разделов. 

Коротко логическое устройство слоев представлено в таблице:
| Слой          | тип запросов      | время отклика запросов | Можно терять данные | Возможен даунтайм  |
| ------------- | ----------------- | ---------------------- | ------------------- | ------------------ |
| Первичка      | исторические      | часы                   | Нет                 | Да                 |
| Аналитический | аналитические     | минуты                 | Да                  | Нежелателен        |
| Витрины       | Заранее известные | секунды                | Нежелательно        | Нет                |


На историческом слое принципиально важно не потерять данные. При этом скорость отдачи данных не так важна, т.к. обращение за данными в этом слое происходят редко: либо для рассчета истории по новой витрине, либо при расследовании инцидентов (которые, опять же, случается редко).

Для аналитического слоя данные всегда можно восстановить. Но времени это занять может долго. Меры надежности стоит принимать в каждой ситуации отдельно (Оценивать Стоимость простоя vs стоимость надежного решения). Мы содержали реплику (кластер: мастер + синхронная реплика + асинхронная реплика).

Данные из презентационного слоя выводятся на UI админки, поэтому критична скорость ответа. Кроме того, нельзя позволить даунтайм этой части системы, т.к. интерфейс останется без данных. При инцидентах лучше показывать старые/необновленные данные, но не показать совсем является неприемлемым. Мы поддерживали кластер с автоматическим фейловером.

## Потоковая обработка событий
Важная часть - как данные попадают в подсистему отчетности и как растекаются по ней. Мы все строили вокруг потока событий. Входной точкой служил брокер очередей RabbitMQ. По кролику проходило разделение зоны ответственности: отправку в rabbit делает продуктовая команда, данные которой будем забирать; чтение из rabbit и последующую обработку делает команда, занимающаяся отчетностью. 

![processor-diag](https://drive.google.com/uc?export=view&id=1RTnHYXbqCgFtCf7n51RFz9or_LARMTAd)

Первый этап - складывание в долгосрочное хранилище. На самом деле вначале данные складывались в промежуточное хранилище. Мы не могли позволить себе хранение данных в rabbit'е. Данные за сутки забили бы железо rabbit'а под завязку. Поэтому мы сначала писали в промежуточное хранилище (хранили там 30 дней). При поступлении события мы собирали связанную информацию, необходимую для расчета аналитики и витрин. Например, при поступлении события об оплате заказа мы докидывали состав заказа. И делали рестрим уже обогащенного события. Все рестримы делались по паттерну [transactional-outbox](https://microservices.io/patterns/data/transactional-outbox.html).

На втором этапе заполнялись таблицы аналитического хранилища. Перед записью в БД по событию производилось много разных расчетов (оплата наличными, сумма скидки, оплата баллами, сумма без скидки, и др.). Обогащенное событие содержало всю необходимую информацию для расчета всех производных показателей. После обработки событие снова рестримилось в rabbit тем же механизмом [transactional-outbox](https://microservices.io/patterns/data/transactional-outbox.html).

На третьем этапе рассчитывались нужные ветрины. Стоит отметить, что для рассчета каждой витрины стоял отдельный обработчик, подписанный на интересные для него события.

Стоит отметить, что все обработчики были написаны, что легко масштабировались горизонтально. Нам хватало 6 инстансов для обработки всего потока и даже для пересчета истории.

## Пересчет истории.
TBD.

## Searcher.

Среди потребностей P была необходимость фильтровать аудиторию: показать на странице, выгрузить, организовать акцию и т.д.
Фильтров было много, фильтры были разнообразны.

Например, типичный запрос мог выглядеть так. Найти гостей:  
- мужского пола, у которых до дня рождения не более двух недель
- на ранге "Новичок"
- заказывавших чай с 01.01.2021 по 31.05.2021 от 3 до 10 раз в ресторанах р1 и р7

Источников было намного больше. Для демонстрации концепции нам хватит и пары.

В указанном примере мы видим 3 разных источника данных: 
- профиль (БД профилей (поход в базу)
- ранг (сервис рангов (обращение по http))
- история заказов (аналитическая БД)

Вторым шагом было действие с гостями. Пользователь мог просто посмотреть список в UI, выгрузить, отправить рассылку, посчитать отчет, назначить тег и пр.

Решение мы так же разделили на 2 части: поиск и применение действия.

Учитывая, что фильтровать гостей мы хотели по параметрам из разных источников, мы реализовали код слияния результатов подзапросов на бэкенд. На вход в API приходил фильтр с большим количеством полей, затем этот фильтр разделялся на параметры отдельных ресурсов. 

Поисковые запросы уходили одновременно во все те ресурсы, фильтры из которых были выбраны пользователем. По конвенции результаты поисковых запросов приходили 
1. потоком (т.е. мы могли начинать читать результаты, пока полный сет гостей еще не сформирован);
2. в отсортированном порядке (т.е. мы могли легко джойнить результаты подзапросов по мере обрабатывания потоков).

![searcher-diag](https://drive.google.com/uc?export=view&id=1xLRSPy3fTujDFA_PeUMk0hzFRYDoivut)

После этого получившийся набор id'шников гостей уходил в слой подготовки результатов. Это могли быть выбор страницы (применение сортировки и выбор страницы, подготовка отчета, выгрузка в excel и др.).

## Физическая архитектура
Все слои мы организовали на кластере SQL Server. Явная граница между аналитическим слоем и слоем витрин отсутствовала. Так получалось, иногда аналитический слой использовался в качестве витрин. На начальном этапе мы не старались строить витрины под все кейсы. Соответственно, потом приходилось достраивать их постепенно.

Сейчас я бы предложил строить разные слои на разных технологиях.
Например,
1. Исторический. Какое-нибудь дешевое объемное хранилище. Обязательно бэкапы или репликация. Пример: hadoop, s3.
2. Аналитический. Колоночная база данных. Пример: ClickHouse, Vertica.
3. Презентационный. Реляционная БД. Пример: SQL Server, PostgreSQL. Хорошим дополнением может стать OLAP.


## Вместо заключения.
Много деталей пропущено. Если какие-то интересны, можно написать мне - я добавлю. 




